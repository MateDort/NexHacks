TapMate Hackathon Execution Plan
1. Project Overview
TapMate is an intelligent Android assistant for the visually impaired that uses a "Brain-Eye-Hand" architecture. It combines Gemini 2.5 Flash (Brain) with Overshoot Live Vision (Eye) and Android Accessibility Service (Hand) to navigate apps and the real world.

Key Innovation: Replacing slow screenshots with fast Accessibility Node Trees and adding a "Smart Memory" to recall critical details (like Uber license plates) after long waits.

2. Architecture & Tech Stack
  Language: Java (Android)
  Brain (LLM): Gemini 2.5 Flash (via Vertex AI Java SDK)
  Why: Fast, cheap, supports Function Calling.
  Eye (Vision): Overshoot Live Vision API
  Why: <300ms latency, real-time object detection (Car, License Plate, Door).
  Hand (Action): Android Accessibility Service
  Why: Direct UI interaction (Click, Scroll), zero-latency state reading (Node Tree).
  Memory (DB): Room Database (SQLite)
  Why: Local, fast, structured storage for "Recall" features.
3. "Trained Agent" Strategy (The Hackathon "Must-Have")
We will not just prompt; we will Fine-Tune to prove technical depth.

Step A: Data Generation
Create training_data.jsonl with ~50 examples mapping Context (Node Tree + User Goal) to Function Call.

  Input: `Goal: "Order Uber to Bakery" | Screen: [Node(id='dest', text='Where to?')]`
  Output: {"tool": "gui_click", "args": {"id": "dest"}}
Step B: Fine-Tuning
Use Google Vertex AI or Google AI Studio to fine-tune gemini-2.5-flash.

  Result: A model ID projects/.../tunedModels/tapmate-v1 that is specialized for your JSON structure.
Step C: Benchmarking & Visualization
Prove improvement using the custom AgentBenchmark.java script.

  Metric 1: Latency (Time to Action).
  Metric 2: Accuracy (Correct Function Call).
  Metric 3: Token Usage.
  Chart: "Trained Agent vs Base Gemini" (Bar Chart showing 3x speedup).
Step D: Comparative Benchmarking (Post-Training)
Once the app is functional, run side-by-side tests against existing solutions:

  Competitors: Be My Eyes (AI Description), Seeing AI.
  Task: "Find the specific Red Prius."
  Measurement: Time to Reach Object.
  Hypothesis: TapMate will be 2x faster because "Active Haptic Guidance" (Hot/Cold) is more intuitive than "Passive Audio Description" ("It's to your left... no, further left").
  Output: A split-screen video demo showing TapMate winning.
4. Sponsor Integration Strategy
Maximize points by integrating relevant sponsors:

| Sponsor | Integration | Hackathon Pitch |
| :--- | :--- | :--- |
| Overshoot | Live Vision API for "Orbit Mode" | "Replaced standard Gemini Vision with Overshoot for <300ms latency." |
| Arize (Phoenix) | Agent Tracing | "Debugged agent thought loops and latency bottlenecks using Phoenix traces." |
| LiveKit | Audio Feedback | "Achieved <200ms audio-to-audio latency for real-time blind navigation." |
| Gemini (Google) | Core Reasoning | "Fine-tuned Gemini 2.5 Flash for specialized accessibility tasks." |
| MongoDB Atlas | Cloud Memory (Optional) | "Syncing user preferences and 'Smart Memory' across devices." (Use if simple to add, else stick to Room). |

Don't Use:

  Solana: Adds unnecessary complexity.
  Wood Wide AI / Kairo / DevSwarm: Stick to the core stack to avoid integration hell.
5. User Flows & "Under the Hood"
Flow A: "The Uber Order" (GUI Agent + Maps)
 User: "Order an Uber to the bakery."
 System (Brain): First calls google_search("bakery near me") to get address.
 System (Brain): Calls `gui_agent("Order Uber to [Found Address]")`.
 System (Hand): Scans Node Tree -> LLM decides click(destination_input) -> Types Address -> Selects ride.
 System (Memory): On confirmation screen, extracts:
  Car: "Red Prius, Plate ABC-123"
  ETA: "5 min"
  Pickup Spot: "Corner of 5th & Main" (from screen or map pin)
 System (Location): Calculates "Walk Time" to Pickup Spot using google_maps(user_loc, pickup_loc).
  Logic: If ETA (5m) > Walk Time (2m), wait 3m.
 User Experience: "Uber ordered. It arrives in 5 minutes. You are 2 minutes away from the pickup spot. I'll tell you when to leave."
 Trigger: When (ETA - Walk Time) < 1 min -> "Leave now to meet your Uber."
Flow B: "The Arrival" (Vision Agent + Memory + Haptics)
 Trigger: 5 mins later (Smart Prediction from ETA).
 System (Brain): Recalls {"color": "red", "type": "Prius", "plate": "ABC"}.
 System (Eye): Activates Overshoot Vision. Configures "Target Lock" on Car.
 System (Feedback):
  Vision Loop (500ms): Checks frame for target.
  Haptics: If car is to the Right, vibrate Right side. If Left, vibrate Left.
  Visual: Draws Neon Green Box over car on screen (Orbit UI).
  Audio: "Slightly right... found it. 5 meters ahead."
Flow C: "The Crosswalk" (Vision + Location + Maps)
 User: "Give me walking directions to Walmart."
 System (Brain): Calls google_maps_directions("Walmart") -> Starts Navigation.
 System (UI): Enters Orbit Mode. Screen black. Neon Green Arrow points to next waypoint.
 System (Location): Monitors distance to next intersection.
 Proactive Alert:
  50 steps away: "Crosswalk in 50 steps. Preparing vision." (Camera activates in background).
  10 steps away: "Approaching crosswalk. Slow down."
  5 steps away: "Stop."
 System (Eye): Analyzes Traffic Light / Traffic.
  If Green + Safe: "Light is Green. Walk." (Arrow appears).
  If Red: "Light is Red. Wait." (Screen Red).
  If No Light: "No light detected. Scan Left... Scan Right." (Guides head movement).
6. Tokenizer Optimization (Cost & Speed)
  Node Tree Pruning: We strip all non-essential Accessibility Nodes (containers, invisible views) before sending to Gemini, reducing payload by ~70%.
  Vision Sampling: Overshoot API is only called when "Triggered" (e.g., at crosswalk or arrival), not continuously, saving tokens and bandwidth.
  Caching: Common UI states (e.g., Uber Home Screen) are cached to avoid re-analyzing known layouts.
7. Resources & API Keys
  Gemini: Google AI Studio (Get API Key).
  Overshoot: Overshoot Dashboard (Get API Key).
  Arize: Arize Phoenix (Get Tracing Key).
  LiveKit: LiveKit Cloud (Get URL/Token).
  Google Maps: GCP Console (Places & Directions API).
  Tokenizer API: (Internal Logic / TikToken) for cost optimization.
8. VC / Investor Evaluation (Realistic Assessment)
  Innovation (4.5/5): Action vs. Description. Existing apps (Be My Eyes, Seeing AI) and hardware (Meta Ray-Ban, Envision) excel at describing the world ("There is a car"). TapMate innovates by acting on it ("I ordered the Uber and I'm guiding you to it"). The "Deep Memory" is a unique differentiator for continuity.
  Technical Execution (5/5): High Complexity. Coordinating Live Vision, Accessibility Trees, and LLM reasoning in real-time (<500ms) is a significant engineering feat, especially with the added Tokenizer/Cost optimizations.
  Impact (5/5): High. 2.2B visually impaired users. Moving from "passive information" to "active assistance" solves the "Last Meter" problem (finding the specific door handle, the specific Uber).
  Design (4/5): Pragmatic. The "Orbit Mode" (Haptic/Audio) is intuitive, but relies heavily on the user holding the phone correctly. Future iterations needs wearables integration.
  Verdict: Strong Market Entry. While not the first vision assistant, TapMate is the first "Agentic" assistant that closes the loop between digital tasks and physical navigation.
9. Future Roadmap: How to Make it Even Better
  Wearable Integration: Move "Orbit Mode" from phone screen to smart glasses (Ray-Ban Meta/Frame) for heads-up haptics/audio.
  Indoor Navigation: Integrate with UWB (Ultra Wideband) or WiFi RTT for precision indoor finding (e.g., finding a specific product on a shelf in Walmart).
  Crowdsourced Maps: Allow users to tag "Safe Crosswalks" or "Tricky Entrances" to improve the system's brain over time.
10. Immediate Next Steps (Your Checklist)
 API Keys:âœ…
  [ ] Get Gemini API Key from Google AI Studio.
  [ ] Get Overshoot API Key.
  [ ] Get Google Maps API Key.
  [ ] add to .env
 Fine-Tuning:
  [ ] Go to Google Vertex AI Console.
  [ ] Upload training/tapmate_gui_training.jsonl.
  [ ] Start Tuning Job (Base model: gemini-2.5-flash).
  [ ] Once done, copy Model ID to GeminiClient.java.
 Build & Deploy:
  [ ] Open TapMate folder in Android Studio.
  [ ] Sync Gradle.
  [ ] Connect Android Device (Enable Developer Mode).
  [ ] Run App.
  [ ] Crucial: Go to Android Settings -> Accessibility -> Enable "TapMate Service".
 Demo Prep:
  [ ] Run python benchmarks/visualize_results.py to get your charts.
  [ ] Record a screen capture of the "Uber Flow" using the app.